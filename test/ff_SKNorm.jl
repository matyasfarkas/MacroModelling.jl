using MacroModelling
import Turing, StatsPlots, Random, Statistics
import LinearAlgebra as ‚Ñí

using HypothesisTests, Distributions

@model RBC begin
    1 / (- k[0]  + (1 - Œ¥ ) * k[-1] + (exp(z[-1]) * k[-1]^Œ±)) = (Œ≤   / (- k[+1]  + (1 - Œ¥) * k[0] +(exp(z[0]) * k[0]^Œ±))) * (Œ±* exp(z[0]) * k[0] ^(Œ± - 1) + (1 - Œ¥))  ;
    #    1 / c[0] - (Œ≤ / c[1]) * (Œ± * exp(z[1]) * k[1]^(Œ± - 1) + (1 - Œ¥)) =0
    #    q[0] = exp(z[0]) * k[0]^Œ± 
    z[0] =  œÅ * z[-1] - œÉ* EPSz[x]
end

@parameters RBC verbose = true begin 
    œÉ = 0.01
    Œ± = 0.25
    Œ≤ = 0.95
    œÅ = 0.2
    Œ¥ = 0.02
    Œ≥ = 1.
end

# draw shocks
Random.seed!(1)
periods = 20
shockdist = Turing.SkewNormal(0,1,2) #  Turing.Beta(10,1) #
shocks = rand(shockdist,1,periods) #  shocks = randn(1,periods)

#shocks /= Statistics.std(shocks)  # antithetic shocks
#shocks .-= Statistics.mean(shocks) # antithetic shocks
# Test for non-normality
        HypothesisTests.ExactOneSampleKSTest(shocks[1,:],Turing.Normal(0,1))
        StatsPlots.plot(Distributions.Normal(0,1), fill=(0, .5,:blue))
        StatsPlots.density!(shocks')
# get simulation
simulated_data = get_irf(RBC,shocks = shocks, periods = 0, levels = true)#(:k,:,:) |>collect

# plot simulation
MacroModelling.plot_irf(RBC,shocks = shocks, periods = 0)
#StatsPlots.plot(shocks')
Œ© = 10^(-5)# eps()
n_samples = 1000


# define loglikelihood model - KF
Turing.@model function loglikelihood_scaling_function(m, data, observables, Œ©)
    #œÉ     ~ MacroModelling.Beta(0.01, 0.02, ŒºœÉ = true)
    # Œ±     ~ MacroModelling.Beta(0.25, 0.15, 0.1, .4, ŒºœÉ = true)
    # Œ≤     ~ MacroModelling.Beta(0.95, 0.05, .9, .9999, ŒºœÉ = true)
    #œÅ     ~ MacroModelling.Beta(0.2, 0.1, ŒºœÉ = true)
    # Œ¥     ~ MacroModelling.Beta(0.02, 0.05, 0.0, .1, ŒºœÉ = true)
    # Œ≥     ~ Turing.Normal(1, 0.05)
    # œÉ     ~ MacroModelling.InverseGamma(0.01, 0.05, ŒºœÉ = true)

    Œ± ~ Turing.Uniform(0.15, 0.45)
    Œ≤ ~ Turing.Uniform(0.92, 0.9999)
    Œ¥ ~ Turing.Uniform(0.0001, 0.1)
    œÉ ~ Turing.Uniform(0.0, 0.1)
    œÅ ~ Turing.Uniform(0.0, 1.0)
    Œ≥ ~ Turing.Uniform(0.0, 1.5)

    # Œ± = 0.25
    # Œ≤ = 0.95
    # œÉ = 0.01
    # œÅ = 0.2
    # Œ¥ = 0.02
    # Œ≥ = 1.

    algorithm = :first_order
    parameters = [œÉ, Œ±, Œ≤, œÅ, Œ¥, Œ≥]

    Turing.@addlogprob! calculate_kalman_filter_loglikelihood(m, data(observables), observables; parameters = parameters)

    # solution = get_solution(m, parameters, algorithm = algorithm)

    # if solution[end] != true
    #     return Turing.@addlogprob! Inf
    # end
    # # draw_shocks(m)
    # x0 ~ Turing.filldist(Turing.Normal(), m.timings.nPast_not_future_and_mixed) # Initial conditions 
    
    # calculate_covariance_ = calculate_covariance_AD(solution[2], T = m.timings, subset_indices = collect(1:m.timings.nVars))

    # long_run_covariance = calculate_covariance_(solution[2])
    
    # initial_conditions = long_run_covariance * x0
    # # initial_conditions = x0

    # ùêí‚ÇÅ = hcat(solution[2][:,1:m.timings.nPast_not_future_and_mixed], zeros(m.timings.nVars), solution[2][:,m.timings.nPast_not_future_and_mixed+1:end])

    # œµ_draw ~ Turing.filldist(shock_distribution, m.timings.nExo * size(data, 2))

    # œµ = reshape(œµ_draw, m.timings.nExo, size(data, 2))

    # state = zeros(typeof(initial_conditions[1]), m.timings.nVars, size(data, 2))

    # aug_state = [initial_conditions
    #             1 
    #             œµ[:,1]]

    # state[:,1] .=  ùêí‚ÇÅ * aug_state# + solution[3] * ‚Ñí.kron(aug_state, aug_state) / 2 

    # for t in 2:size(data, 2)
    #     aug_state = [state[m.timings.past_not_future_and_mixed_idx,t-1]
    #                 1 
    #                 œµ[:,t]]

    #     state[:,t] .=  ùêí‚ÇÅ * aug_state# + solution[3] * ‚Ñí.kron(aug_state, aug_state) / 2 
    # end

    # observables_index = sort(indexin(observables, m.timings.var))
    
    # state_deviations = data - state[observables_index,:] .- solution[1][observables_index...]

    # Turing.@addlogprob! sum([Turing.logpdf(Turing.MvNormal(Œ© * ‚Ñí.I(size(data,1))), state_deviations[:,t]) for t in 1:size(data, 2)])
end

loglikelihood_scaling = loglikelihood_scaling_function(RBC, simulated_data(:,:,:Shock_matrix), [:k], Œ©) # Kalman
samps = Turing.sample(loglikelihood_scaling, Turing.NUTS(), n_samples, progress = true)#, init_params = sol


StatsPlots.plot(samps)
kf_estimated_parameters = Turing.describe(samps)[1].nt.parameters
kf_estimated_means = Turing.describe(samps)[1].nt.mean
kf_estimated_std = Turing.describe(samps)[1].nt.std
kfmean= kf_estimated_means
kfstd = kf_estimated_std 
Turing.@model function loglikelihood_scaling_function_ff(m, data, observables, Œ©) #, kfmean, kfstd
     
    #  œÉ     ~ MacroModelling.Beta(0.01, 0.02, ŒºœÉ = true)
    #  Œ±     ~ MacroModelling.Beta(0.25, 0.15, 0.1, .4, ŒºœÉ = true)
    #  Œ≤     ~ MacroModelling.Beta(0.95, 0.05, .9, .9999, ŒºœÉ = true)
    #  œÅ     ~ MacroModelling.Beta(0.2, 0.1, ŒºœÉ = true)
    #  Œ¥     ~ MacroModelling.Beta(0.02, 0.05, 0.0, .1, ŒºœÉ = true)
    #  Œ≥     ~ Turing.Normal(1, 0.05)
    #œÉ     ~ MacroModelling.InverseGamma(0.01, 0.05, ŒºœÉ = true)

    Œ± ~ Turing.Uniform(0.15, 0.45)
    Œ≤ ~ Turing.Uniform(0.92, 0.9999)
    Œ¥ ~ Turing.Uniform(0.0001, 0.1)
    œÉ ~ Turing.Uniform(0.0, 0.1)
    œÅ ~ Turing.Uniform(0.0, 1.0)
    Œ≥ ~ Turing.Uniform(0.0, 1.5)
    DF ~ Turing.Uniform(0, 4)
    #Œ± ~ Turing.Uniform(kfmean[1]-2*kfstd[1], kfmean[1]+2*kfstd[1])
    #Œ≤ ~ Turing.Uniform(kfmean[2]-2*kfstd[2], kfmean[2]+2*kfstd[2])
    #Œ¥ ~ Turing.Uniform(kfmean[3]-2*kfstd[3], kfmean[3]+2*kfstd[3])
    #œÉ ~ Turing.Uniform(0.0, kfmean[4]+2*kfstd[4])
    #œÅ ~ Turing.Uniform(0.0, kfmean[5]+2*kfstd[5])
    #Œ≥ ~ Turing.Uniform(0.0, kfmean[6]+2*kfstd[6])


    # Œ± = 0.25
    # Œ≤ = 0.95
    # œÉ = 0.01
    # œÅ = 0.2
    # Œ¥ = 0.02
    # Œ≥ = 1.

    algorithm = :first_order
    parameters = [œÉ, Œ±, Œ≤, œÅ, Œ¥, Œ≥]
    # skewness
    shock_distribution = Turing.SkewNormal(0,1,DF)

    # Turing.@addlogprob! calculate_kalman_filter_loglikelihood(m, data(observables), observables; parameters = parameters)

     solution = get_solution(m, parameters, algorithm = algorithm)

     if solution[end] != true
         return Turing.@addlogprob! Inf
     end
    # draw_shocks(m)
     x0 ~ Turing.filldist(Turing.Normal(), m.timings.nPast_not_future_and_mixed) # Initial conditions 
    
     calculate_covariance_ = MacroModelling.calculate_covariance_AD(solution[2], T = m.timings, subset_indices = collect(1:m.timings.nVars))

     long_run_covariance = calculate_covariance_(solution[2])
    
     initial_conditions = long_run_covariance * x0
    # # initial_conditions = x0

     ùêí‚ÇÅ = hcat(solution[2][:,1:m.timings.nPast_not_future_and_mixed], zeros(m.timings.nVars), solution[2][:,m.timings.nPast_not_future_and_mixed+1:end])
    œµ_draw ~ Turing.filldist(shock_distribution, m.timings.nExo * size(data, 2))

     œµ = reshape(œµ_draw, m.timings.nExo, size(data, 2))

     state = zeros(typeof(initial_conditions[1]), m.timings.nVars, size(data, 2))

     aug_state = [initial_conditions
                 1 
                 œµ[:,1]]

     state[:,1] .=  ùêí‚ÇÅ * aug_state# + solution[3] * ‚Ñí.kron(aug_state, aug_state) / 2 

     for t in 2:size(data, 2)
         aug_state = [state[m.timings.past_not_future_and_mixed_idx,t-1]
                     1 
                     œµ[:,t]]

         state[:,t] .=  ùêí‚ÇÅ * aug_state# + solution[3] * ‚Ñí.kron(aug_state, aug_state) / 2 
     end

     observables_index = sort(indexin(observables, m.timings.var))
    
     state_deviations = data - state[observables_index,:] .- solution[1][observables_index...]

     Turing.@addlogprob! sum([Turing.logpdf(Turing.MvNormal(Œ© * ‚Ñí.I(size(data,1))), state_deviations[:,t]) for t in 1:size(data, 2)])
end

loglikelihood_scaling_ff = loglikelihood_scaling_function_ff(RBC, collect(simulated_data(:k,:,:Shock_matrix))', [:k], Œ©) # ,kf_estimated_means, kf_estimated_std  # Filter free

n_samples = 5000
samps_ff = Turing.sample(loglikelihood_scaling_ff, Turing.NUTS(), n_samples, progress = true)#, init_params = sol
StatsPlots.plot(samps_ff)

ff_estimated_parameters = Turing.describe(samps_ff)[1].nt.parameters
ff_estimated_means = Turing.describe(samps_ff)[1].nt.mean
ff_estimated_std = Turing.describe(samps_ff)[1].nt.std


ff_bias= ( ff_estimated_means[1:6]- RBC.parameter_values[[2, 3, 5, 1, 4,6]])
kf_bias= ( kf_estimated_means[1:6]- RBC.parameter_values[[2, 3, 5, 1, 4,6]])

ff_z = (ff_bias)./ff_estimated_std[1:6] 
kf_z = ( kf_bias)./kf_estimated_std[1:6] 

grouplabel = repeat(["KF", "FF"], inner = 6)

StatsPlots.groupedbar( repeat(kf_estimated_parameters, outer =2) , [kf_bias ff_bias], group = grouplabel, xlabel = "Structural Parameters Biases")
StatsPlots.groupedbar( repeat(kf_estimated_parameters, outer =2), [kf_z ff_z], group = grouplabel, xlabel = "Structural Parameter z-scores")
data = KeyedArray(Array(collect(simulated_data(:k,:,:Shock_matrix)))',row = [:k], col = 1:1:20)



kf_filtered_shocks = MacroModelling.get_estimated_shocks(RBC, data, parameters = kf_estimated_means[[4, 1, 2, 5, 3,6]])


ff_estimated_parameters_indices = indexin([Symbol("œµ_draw[$a]") for a in 1:periods], ff_estimated_parameters )
StatsPlots.plot(ff_estimated_means[ff_estimated_parameters_indices],
                ribbon = 1.96 * ff_estimated_std[ff_estimated_parameters_indices], 
                label = "Posterior mean", 
                title = "Joint: Estimated Latents")
StatsPlots.plot!(shocks', label = "True values")
StatsPlots.plot!(collect(kf_filtered_shocks'), label = "KF filtered shocks")


StatsPlots.plot(samps_ff[["DF"]]; colordim=:parameter, legend=true)
